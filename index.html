<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Golsa Tahmasebzadeh</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.2/css/bulma.min.css">
  <script defer src="https://use.fontawesome.com/releases/v5.3.1/js/all.js"></script>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-B880VEV8XB"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-B880VEV8XB');
  </script>
  
</head>

<body>
<section class="section mx-6">
  <div class="columns">
  <div class="column is-2 is-flex" style="justify-content: center; align-items: center;">
<!--       <figure class="image is-128x128">
        <img class="is-rounded" src="self.png">
      </figure> -->
    </div> 
    <div class="column has-text-centered-mobile">
      <h1 class="title">
        Golsa Tahmasebzadeh
      </h1>
      <p class="content">
        <p>
          <i class="fas fa-map-marker-alt"></i> Hannover, Germany
       </p>
        <p>
          Research Assistant at Leibniz Information Center for Science and Technology (TIB) and L3S Research Center
        </p>
        <p>
          PhD Candidate at the Gottfried Wilhelm Leibniz University of Hannover
        </p>
        <br>
        <p>
            <i class="fas fa-envelope"></i> Email me: <a href="mailto:golsatahm@gmail.com">golsatahm@gmail.com</a>
        </p>
      </p>
    </div>
  </div>
</section>

<div class="section mx-6">
  <div class="columns">
    <div class="column is-2 has-text-right has-text-centered-mobile">
    </div>
    <div class="column">
      <div class="content">
        <p>
         My PhD is focused on multimodal analytics of multilingual news articles, with a specific emphasis on inferring and distinguishing geospatial, events and temporal information within news narratives. My objective is to employ advanced multimodal machine learning techniques to effectively bridge the gaps between visual and textual modalities. Furthermore, I aim to enrich news documents by integrating external knowledge derived from knowledge graphs, thereby enhancing the contextualization of news content. The contextualization is important to organize and analyze news documents in various applications such as fake news detection, fact checking, and news retrieval. 
        </p>
        <p>
          As an Early Stage Researcher (ESR) in the CLEOPATRA ITN, a Marie Sk≈Çodowska-Curie Innovative Training Network, I was responsible for facilitating advanced cross-lingual processing of event-centric textual and visual information through development of novel methods for extraction, verification and contextualisation of multilingual information. Additionally, my role in the FakeNarratives project aimed at understanding narratives of disinformation in public and alternative news videos has empowered me to directly confront one of the most significant real-world challenges.
        </p>
      </div>
    </div>
    <div class="column is-1"></div>
  </div>
</div>

  <div class="section mx-6">
    <div class="columns">
      <div class="column is-2 has-text-right has-text-centered-mobile">
        <p class="subtitle">Education</p>
      </div>
      <div class="column">
          <div class="content">
            <table>
              <tbody>
                <tr>
                  <td></td>
                </tr>
                <tr>
                  <td>
                    <strong>PhD Candidate</strong> <br>
                    <small>Aug. 2019 - present <br>
                    <i>Gottfried Wilhelm Leibniz University of Hannover</i></small>
                  </td>
                </tr>
                <tr>
                  <td>
                    <strong>MSc in Computer Engineering - Artificial Intelligence and Robotics </strong> <br>
                    <small>Sep. 2016 - Feb. 2019 <br>
                      <i>Faculty of Electrical and Computer Engineering, University of Tabriz</i></small>
                    
                  </td>
                </tr>
                <tr>
                  <td>
                    <strong>BSc in Information Technology Engineering</strong> <br>
                    <small>
                      Sep. 2012 - Aug. 2016 <br>
                      <i>Faculty of Electrical and Computer Engineering, University of Tabriz</i>
                    </small>
                  </td>
                </tr>
                <tr>
                  <td></td>
                </tr>
              </tbody>
            </table>
          </div>
      </div>
      <div class="column is-1"></div>
    </div>
  </div>




  
  <section class="section mx-6">
      <div class="columns">
        <div class="column is-2 has-text-right has-text-centered-mobile">
          <p class="subtitle">Projects Overview</p>
        </div>
        
        <div class="column" style="width: 100%;">
          <div class="content">
          <table>
            <tbody>
                    <tr>
                      <td style="width: 50%;">
                         <strong>Few-Shot Event Classification in Images using Knowledge Graphs for Prompting</strong> <br> <br>
                         <img src="images/EIP.png" style="width: 95%;" class="center" >
                         Event classification in images plays a vital role in multimedia analysis especially with the prevalence of fake news on social media and the Web. The majority of approaches for event classification rely on large sets of labeled training data. However, image labels for fine-grained event instances (e.g., 2016 Summer Olympics) can be sparse, incorrect, ambiguous, etc. A few approaches have addressed the lack of labeled data for event classification but cover only few events. Moreover, vision-language models that allow for zero-shot and few-shot classification with prompting have not yet been extensively exploited. In this paper, we propose four different techniques to create hard prompts including knowledge graph information from Wikidata and Wikipedia as well as an ensemble approach for zero-shot event classification. We also integrate prompt learning for state-of-the-art vision-language models to address few-shot event classification. Experimental results on six benchmarks including a new dataset comprising event instances from various domains, such as politics and natural disasters, show that our proposed approaches require much fewer training images than supervised baselines and the state-of-the-art while achieving better results. 
                         <br> <br>
                          <b>Paper link: </b><a href="https://openaccess.thecvf.com/content/WACV2024/papers/Tahmasebzadeh_Few-Shot_Event_Classification_in_Images_Using_Knowledge_Graphs_for_Prompting_WACV_2024_paper.pdf"  target="_blank">Few-Shot Event Classification in Images using Knowledge Graphs for Prompting</a>
                          <br>
                          <b>Github link: </b><a href="https://github.com/TIBHannover/PromptImageEvent"  target="_blank">https://github.com/TIBHannover/PromptImageEvent</a>
                      </td>
                      
                      <td>
                          <strong> GeoWINE: Geolocation based Wiki, Image, News and Event Retrieval </strong> <br> <br>
                         <img src="images/geowine.png" style="width: 80%;" class="center" alt="" >
                        <br>
                        In the context of social media, geolocation inference on news or events has become a very important task. In this paper, we present the GeoWINE (Geolocation-based Wiki-Image-News-Event retrieval) demonstrator, an effective modular system for multimodal retrieval which expects only a single image as input. The GeoWINE system consists of five modules in order to retrieve related information from various sources. The first module is a state-of-the-art model for geolocation estimation of images. The second module performs a geospatial-based query for entity retrieval using the Wikidata knowledge graph. The third module exploits four different image embedding representations, which are used to retrieve most similar entities compared to the input image. The last two modules perform news and event retrieval from EventRegistry and the Open Event Knowledge Graph (OEKG). GeoWINE provides an intuitive interface for end-users and is insightful for experts for reconfiguration to individual setups. The GeoWINE achieves promising results in entity label prediction for images on Google Landmarks dataset. 
                         <br> <br>
                          <b>Paper link: </b><a href="https://dl.acm.org/doi/abs/10.1145/3404835.3462786?casa_token=-E6lOzxlRd8AAAAA:pZ8VnOFSjNReKlsaME2a0L2DM6JZArkqEorJWUysjvt-5p8V7GVegM3kId71idSkIZM4YCOy0G8gsMM"  target="_blank">GeoWINE: Geolocation based Wiki, Image, News and Event Retrieval</a>
                          <br>
                          <b>Github link: </b><a href="https://github.com/cleopatra-itn/GeoWINE"  target="_blank">https://github.com/cleopatra-itn/GeoWINE</a>
                          <br>
                          <b>Demo link: </b> <a href="http://cleopatra.ijs.si/geowine/">http://cleopatra.ijs.si/geowine/</a>
                      </td>
                    </tr>
                    <tr>
                      <td style="width: 50%;">
                         <strong>Multimodal Geolocation Estimation of News Photos</strong> <br> <br>
                         <img src="images/MMGNewsPhoto.png" style="width: 80%;" class="center" alt="" >
                         <br>
                         The widespread growth of multimodal news requires sophisticated approaches to interpret content and relations of different modalities. Images are of utmost importance since they represent a visual gist of the whole news article. For example, it is essential to identify the locations of natural disasters for crisis management or to analyze political or social events across the world. In some cases, verifying the location(s) claimed in a news article might help human assessors or fact-checking efforts to detect misinformation, i.e., fake news. Existing methods for geolocation estimation typically consider only a single modality, e.g., images or text. However, news images can lack sufficient geographical cues to estimate their locations, and the text can refer to various possible locations. In this paper, we propose a novel multimodal approach to predict the geolocation of news photos. To enable this approach, we introduce a novel dataset called Multimodal Geolocation Estimation of News Photos (MMG-NewsPhoto). MMG-NewsPhoto is, so far, the largest dataset for the given task and contains more than half a million news texts with the corresponding image, out of which 3000 photos were manually labeled for the photo geolocation based on information from the image-text pairs. For a fair comparison, we optimize and assess state-of-the-art methods using the new benchmark dataset. Experimental results show the superiority of the multimodal models compared to the unimodal approaches.
                          <br> <br>
                          <b>Paper link: </b><a href="https://www.repo.uni-hannover.de/bitstream/handle/123456789/15014/ECIR_2023___MMG_News__LUH.pdf?sequence=1&isAllowed=y"  target="_blank">Multimodal Geolocation Estimation of News Photos</a>
                          <br>
                          <b>Github link: </b><a https://github.com/TIBHannover/mmg-newsphoto"  target="_blank">https://github.com/TIBHannover/mmg-newsphoto</a>
                      </td>
                      <td ">
                         <strong>MLM: A benchmark dataset for multitask learning with multiple languages and modalities</strong> <br> <br>
                         <img src="images/MLM.png" style="width: 80%;" class="center" alt="" >
                         <br>
                         In this paper, we introduce the MLM (Multiple Languages and Modalities) dataset - a new resource to train and evaluate multitask systems on samples in multiple modalities and three languages. The generation process and inclusion of semantic data provide a resource that further tests the ability for multitask systems to learn relationships between entities. The dataset is designed for researchers and developers who build applications that perform multiple tasks on data encountered on the web and in digital archives. A second version of MLM provides a geo-representative subset of the data with weighted samples for countries of the European Union. We demonstrate the value of the resource in developing novel applications in the digital humanities with a motivating use case and specify a benchmark set of tasks to retrieve modalities and locate entities in the dataset. Evaluation of baseline multitask and single task systems on the full and geo-representative versions of MLM demonstrate the challenges of generalising on diverse data. In addition to the digital humanities, we expect the resource to contribute to research in multimodal representation learning, location estimation, and scene understanding.
                        <br> <br>
                          <b>Paper link: </b><a href="https://arxiv.org/pdf/2008.06376.pdf"  target="_blank">MLM: A Benchmark Dataset for Multitask Learning with Multiple Languages and Modalities</a>
                          <br>
                          <b>Github link: </b><a https://github.com/GOALCLEOPATRA/MLM"  target="_blank">https://github.com/GOALCLEOPATRA/MLM</a>
                      </td>
                    </tr>
                    <tr>
                      <td style="width: 50%;">
                         <strong>MM-Locate-News: Multimodal Focus Location Estimation in News</strong> <br> <br>
                         <img src="images/MM-LocateNews.png" style="width: 80%;" alt="" >
                         <br>
                         The consumption of news has changed significantly as the Web has become the most influential medium for information. To analyze and contextualize the large amount of news published every day, the geographic focus of an article is an important aspect in order to enable content-based news retrieval. There are methods and datasets for geolocation estimation from text or photos, but they are typically considered as separate tasks. However, the photo might lack geographical cues and text can include multiple locations, making it challenging to recognize the focus location using a single modality. In this paper, a novel dataset called Multimodal Focus Location of News (MM-Locate-News) is introduced. We evaluate state-of-the-art methods on the new benchmark dataset and suggest novel models to predict the focus location of news using both textual and image content. The experimental results show that the multimodal model outperforms unimodal models.
                        <br> <br>
                          <b>Paper link: </b><a href="https://arxiv.org/pdf/2211.08042"  target="_blank">MM-Locate-News: Multimodal Focus Location Estimation in News</a>
                          <br>
                          <b>Github link: </b><a https://github.com/TIBHannover/mm-locate-news"  target="_blank">https://github.com/TIBHannover/mm-locate-news</a>
                      </td>
                      <td>Row 3, Cell 2</td>
                    </tr>
            </tbody>
            
          </table>
          </div>
        </div>
          
      </div>
      </div>
  </section>



  

  
  <section class="section mx-6">
    <div class="columns">
      <div class="column is-2 has-text-right has-text-centered-mobile">
        <p class="subtitle">Publications</p>
      </div>
      <div class="column">
        <div class="content">
           <script src="https://bibbase.org/show?bib=https://dblp.org/pid/270/1710.bib&noBootstrap=1&jsonp=1"></script>
        </div>
      </div>
      <div class="column is-1"></div>
    </div>
  </section>

  

  
-->
  </body>
</html>
